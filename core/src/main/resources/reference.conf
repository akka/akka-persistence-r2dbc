
// #journal-settings
akka.persistence.r2dbc {
  journal {
    class = "akka.persistence.r2dbc.journal.R2dbcJournal"

    # name of the table to use for events
    table = "event_journal"

    # the column type to use for event payloads (BYTEA or JSONB)
    payload-column-type = "BYTEA"

    # Otherwise it would be a pinned dispatcher, see https://github.com/akka/akka/issues/31058
    plugin-dispatcher = "akka.actor.default-dispatcher"

    # event replay is using akka.persistence.r2dbc.query.buffer-size

    # Set this to off to disable publishing of of events as Akka messages to running
    # eventsBySlices queries.
    # Tradeoff is more CPU and network resources that are used. The events
    # must still be retrieved from the database, but at a lower polling frequency,
    # because delivery of published messages are not guaranteed.
    # When this feature is enabled it will measure the throughput and automatically
    # disable/enable if the throughput exceeds the configured threshold. See
    # publish-events-dynamic configuration.
    publish-events = on

    # When publish-events is enabled it will measure the throughput and automatically
    # disable/enable if the throughput exceeds the configured threshold.
    # This configuration cannot be defined per journal, but is global for the ActorSystem.
    publish-events-dynamic {
      # If exponentially weighted moving average of measured throughput exceeds this
      # threshold publishing of events is disabled. It is enabled again when lower than
      # the threshold.
      throughput-threshold = 400
      # The interval of the throughput measurements.
      throughput-collect-interval = 10 seconds
    }

    # Group the slices for an entity type into this number of topics. Most efficient is to use
    # the same number as number of projection instances. If configured to less than the number of
    # of projection instances the overhead is that events will be sent more than once and discarded
    # on the destination side. If configured to more than the number of projection instances
    # the events will only be sent once but there is a risk of exceeding the limits of number
    # of topics that PubSub can handle (e.g. OversizedPayloadException).
    # Must be between 1 and 1024 and a whole number divisor of 1024 (number of slices).
    # This configuration can be changed in a rolling update, but there might be some events
    # that are not delivered via the pub-sub path and instead delivered later by the queries.
    # This configuration cannot be defined per journal, but is global for the ActorSystem.
    publish-events-number-of-topics = 128

    # replay filter not needed for this plugin
    replay-filter.mode = off
  }
}
// #journal-settings

// #snapshot-settings
akka.persistence.r2dbc {
  snapshot {
    class = "akka.persistence.r2dbc.snapshot.R2dbcSnapshotStore"
    table = "snapshot"

    # the column type to use for snapshot payloads (bytea or jsonb)
    payload-column-type = "BYTEA"

    # Otherwise it would be a pinned dispatcher, see https://github.com/akka/akka/issues/31058
    plugin-dispatcher = "akka.actor.default-dispatcher"

    # Enables an optimization in Akka for avoiding snapshot deletes in retention.
    only-one-snapshot = true
  }
}
// #snapshot-settings

// #durable-state-settings
akka.persistence.r2dbc {
  # Durable state store
  state {
    class = "akka.persistence.r2dbc.state.R2dbcDurableStateStoreProvider"

    table = "durable_state"

    # the column type to use for durable state payloads (bytea or jsonb)
    payload-column-type = "BYTEA"

    # When this is enabled the updates verifies that the revision is +1 of
    # previous revision. There might be a small performance gain if
    # this is disabled.
    assert-single-writer = on

    # Extract a field from the state and store in an additional database column.
    # Primary use case is for secondary indexes that can be queried.
    # Each entity type can have several additional columns.
    # The AdditionalColumn implementation may optionally define an ActorSystem
    # constructor parameter.
    additional-columns {
      #"<entity-type-name>" = ["<fqcn of AdditionalColumn implementation>"]
    }

    # Use another table for the given entity types. Typically used together with
    # additional-columns but can also be used without addition-columns.
    custom-table {
      #"<entity-type-name>" =  <other_durable_state_table>
    }

    # Additional processing in the same transaction as the Durable State upsert
    # or delete. Primary use case is for storing a query or aggregate representation
    # in a separate table.
    # The ChangeHandler implementation may optionally define an ActorSystem
    # constructor parameter.
    change-handler {
      #<entity-type-name>" = "<fqcn of ChangeHandler implementation>"
    }

  }
}
// #durable-state-settings

// #query-settings
akka.persistence.r2dbc {
  query {
    class = "akka.persistence.r2dbc.query.R2dbcReadJournalProvider"

    # When live queries return no results or <= 10% of buffer-size, the next query
    # to db will be delayed for this duration.
    # When the number of rows from previous query is >= 90% of buffer-size, the next
    # query will be emitted immediately.
    # Otherwise, between 10% - 90% of buffer-size, the next query will be delayed
    # for half of this duration.
    refresh-interval = 3s

    # Live queries read events up to this duration from the current database time.
    behind-current-time = 100 millis

    backtracking {
      enabled = on
      # Backtracking queries will look back for this amount of time. It should
      # not be larger than the akka.projection.r2dbc.offset-store.time-window.
      window = 2 minutes
      # Backtracking queries read events up to this duration from the current database time.
      behind-current-time = 10 seconds
    }

    # In-memory buffer holding events when reading from database.
    buffer-size = 1000

    persistence-ids {
      buffer-size = 1000
    }

    # When journal publish-events is enabled a best effort deduplication can be enabled by setting
    # this property to the size of the deduplication buffer in the `eventsBySlices` query.
    # It keeps track of this number of entries and 5000 is recommended capacity. The drawback
    # of enabling this is that when the sequence numbers received via publish-events are out of sync
    # after some error scenarios it will take longer to receive those events, since it will rely on
    # the backtracking queries.
    deduplicate-capacity = 0

    # Settings for eventsBySlicesStartingFromSnapshots and currentEventsBySlicesStartingFromSnapshots
    start-from-snapshot {
      # Set this to on true if eventsBySlicesStartingFromSnapshots or
      # currentEventsBySlicesStartingFromSnapshots are used. That has a small overhead when storing
      # snapshots because the timestamp and tags of the corresponding event is retrieved when storing
      # a snapshot.
      # See also https://doc.akka.io/docs/akka-persistence-r2dbc/current/migration-guide.html#eventsBySlicesStartingFromSnapshots
      enabled = false
    }
  }
}
// #query-settings

akka.persistence.r2dbc {
  # Configuration of the Cleanup tool.
  cleanup {
    # Log progress after this number of delete operations. Can be set to 1 to log
    # progress of each operation.
    log-progress-every = 100
    # For large journals deleting events in a single transaction might not be very efficient.
    # Set this value to expected delete batch size to minimize table lock holding and contention.
    events-journal-delete-batch-size = 1000
  }
}

// #connection-settings
akka.persistence.r2dbc {

  # set this to your database schema if applicable, empty by default
  schema = ""

  # Shared defaults, override in the concrete [dialect.connection-factory] you are using
  default-connection-pool {
    // #connection-pool-settings
    # Initial pool size.
    initial-size = 5
    # Maximum pool size.
    max-size = 20

    # Maximum idle time of the connection in the pool.
    # Background eviction interval of idle connections is derived from this property
    # and max-life-time.
    max-idle-time = 30 minutes

    # Maximum lifetime of the connection in the pool.
    # Background eviction interval of connections is derived from this property
    # and max-idle-time.
    max-life-time = 60 minutes

    # Maximum time to acquire connection from pool.
    acquire-timeout = 5 seconds
    # Number of retries if the connection acquisition attempt fails.
    # In the case the database server was restarted all connections in the pool will
    # be invalid. To recover from that without failed acquire you can use the same number
    # of retries as max-size of the pool
    acquire-retry = 1

    # Validate the connection when acquired with this SQL.
    # Enabling this has some performance overhead.
    # A fast query for Postgres is "SELECT 1"
    validation-query = ""

    # Maximum SQL statement execution duration. The current connection is closed if exceeded,
    # and will not be reused by the pool.
    # This timeout is handled on the client side and should be used in case the database server
    # is unresponsive or the connection is broken but not closed.
    # It can be used in combination with `statement-timeout`, which should be less than this
    # timeout.
    # The timeout is needed to handle some failure scenarios, when the database server is
    # terminated in a non graceful way and there is a load balancer in front. The client
    # connection and current execution in progress would not be completed without this timeout,
    # resulting in a "connection leak".
    # Set to "off" to disable this timeout.
    close-calls-exceeding = 20 seconds
    // #connection-pool-settings
  }

  # dialect specific connection factory configurations,
  # assign block to 'connection-factory' to chose which one to actually use.

  # Defaults for postgres
  postgres = ${akka.persistence.r2dbc.default-connection-pool}
  postgres {
    dialect = "postgres"
    driver = "postgres"

    // #connection-settings-postgres
    // #connection-settings-yugabyte
    # the connection can be configured with a url, eg: "r2dbc:postgresql://<host>:5432/<database>"
    url = ""

    # The connection options to be used. Ignored if 'url' is non-empty
    host = "localhost"
    // #connection-settings-yugabyte
    port = 5432
    database = "postgres"
    user = "postgres"
    password = "postgres"
    // #connection-settings-yugabyte

    ssl {
      enabled = off
      # See PostgresqlConnectionFactoryProvider.SSL_MODE
      # Possible values:
      #  allow - encryption if the server insists on it
      #  prefer - encryption if the server supports it
      #  require - encryption enabled and required, but trust network to connect to the right server
      #  verify-ca - encryption enabled and required, and verify server certificate
      #  verify-full - encryption enabled and required, and verify server certificate and hostname
      #  tunnel - use a SSL tunnel instead of following Postgres SSL handshake protocol
      mode = ""

      # Server root certificate. Can point to either a resource within the classpath or a file.
      root-cert = ""

      # Client certificate. Can point to either a resource within the classpath or a file.
      cert = ""

      # Key for client certificate. Can point to either a resource within the classpath or a file.
      key = ""

      # Password for client key.
      password = ""
    }

    # Maximum time to create a new connection.
    connect-timeout = 3 seconds

    # Configures the statement cache size.
    # 0 means no cache, negative values will select an unbounded cache
    # a positive value will configure a bounded cache with the passed size.
    statement-cache-size = 5000

    # Abort any statement that takes more than the specified amount of time.
    # This timeout is handled by the database server.
    # This timeout should be less than `close-calls-exceeding`.
    statement-timeout = off
    // #connection-settings-postgres
    // #connection-settings-yugabyte
  }

  # Defaults for yugabyte
  yugabyte = ${akka.persistence.r2dbc.postgres}
  yugabyte {
    dialect = "yugabyte"
    // #connection-settings-yugabyte

    port = 5433
    database = "yugabyte"
    user = "yugabyte"
    password = "yugabyte"
    // #connection-settings-yugabyte
  }

  # Defaults for H2
  h2 = ${akka.persistence.r2dbc.default-connection-pool}
  h2 {

    dialect = "h2"
    // #connection-settings-h2
    # the connection can be configured with a url, eg: 'r2dbc:h2:file:////some/path/db-file;DB_CLOSE_DELAY=-1'
    url = ""
    # 'file' or 'mem' default is mem, separate process over 'tcp' is not supported
    protocol = "mem"
    # For mem protocol, symbolic database name, all interactions with the same name in the same process see the same data,
    # For file protocol, relative or absolute file path to where database is kept on disk
    # Note: for file based the file cannot be shared by multiple JVMs, it must be one unique path per process
    database = "akka-r2dbc"
    # Enable trace logging via slf4j, logger used is named 'h2database'
    # http://www.h2database.com/html/features.html#trace_options
    # (Enabling may come with a performance overhead)
    trace-logging = off
    # DDL or DML included in init together with plugin table creation
    # http://www.h2database.com/html/features.html#execute_sql_on_connection
    additional-init=""

    # Additional H2 options to use when creating the database.
    # See https://h2database.com/javadoc/org/h2/engine/DbSettings.html for a list of available options
    additional-options {
    }

    # Create database indexes to optimize slice queries on the journal and durable state
    # has a slight overhead so can be disabled if not using the slice queries
    create-slice-indexes = true

    # The H2 driver blocks in suprising places, run potentiall blocking tasks on this dispatcher,
    # can be configured to be a specific dispatcher but note that since it will be blocking it should
    # not be together with other async tasks.
    use-dispatcher = "akka.actor.default-blocking-io-dispatcher"

    # Note: hack to pull in custom table names into schema creation in H2 connection properties
    # for custom/multi journal configs where table and chema is changed and plugin config is not under
    # 'akka.persistence.r2dbc' these have to be re-pointed to the actual config location
    schema = ${akka.persistence.r2dbc.schema}
    journal-table = ${akka.persistence.r2dbc.journal.table}
    state-table = ${akka.persistence.r2dbc.state.table}
    snapshot-table = ${akka.persistence.r2dbc.snapshot.table}
    // #connection-settings-h2
  }

  # Defaults for SQL Server
  sqlserver = ${akka.persistence.r2dbc.default-connection-pool}
  sqlserver {
    dialect = "sqlserver"
    driver = "mssql"

    // #connection-settings-sqlserver
    # the connection can be configured with a url, eg: "r2dbc:sqlserver://<host>:1433/<database>"
    url = ""

    # The connection options to be used. Ignored if 'url' is non-empty
    host = "localhost"

    port = 1433
    database = "master"
    user = "SA"
    password = "<YourStrong@Passw0rd>"

    # Maximum time to create a new connection.
    connect-timeout = 3 seconds

    # Used to encode tags to and from db. Tags must not contain this separator.
    tag-separator = ","

    // #connection-settings-sqlserver
  }


  # Assign the connection factory for the dialect you want to use, then override specific fields
  # connection-factory = ${akka.persistence.r2dbc.postgres}
  # connection-factory {
  #   host = "myhost.example.com"
  # }
  connection-factory = {}

  # If database timestamp is guaranteed to not move backwards for two subsequent
  # updates of the same persistenceId there might be a performance gain to
  # set this to `on`. Note that many databases use the system clock and that can
  # move backwards when the system clock is adjusted.
  # Ignored for H2 and sqlserver
  db-timestamp-monotonic-increasing = off

  # Enable this to generate timestamps from the Akka client side instead of using database timestamps.
  # NTP or similar clock synchronization should be used, but some clock skew between clients is accepted.
  # Ignored for H2 and sqlserver
  use-app-timestamp = off

  # Logs database calls that take longer than this duration at INFO level.
  # Set to "off" to disable this logging.
  # Set to 0 to log all calls.
  log-db-calls-exceeding = 300 ms

}
// #connection-settings
